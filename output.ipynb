{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 317.0 MB 105 kB/s eta 0:00:017    |██████████████                  | 139.5 MB 3.1 MB/s eta 0:00:58     |████████████████████████████▋   | 283.7 MB 2.5 MB/s eta 0:00:14\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488513 sha256=8e994048a4e2e2cb57aa025c3b43a900dc43f3a7e521aaccd12a6b6334842bad\n",
      "  Stored in directory: /Users/muriel/Library/Caches/pip/wheels/da/78/6d/54350e0243f65f77dccf6ebe2ed5559faf6900559e904fb957\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import yaml\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BCG_Crash_Analysis\").getOrCreate()\n",
    "primary_person_df = spark.read.csv('data/Primary_Person_use.csv', header=True)\n",
    "units_df = spark.read.csv('data/Units_use.csv', header=True)\n",
    "damages_df = spark.read.csv('data/Damages_use.csv', header=True)\n",
    "charges_df = spark.read.csv('data/Charges_use.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/muriel/Documents/Github/case_study_bcg/data/data/Primary_Person_use.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-95035000f5bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\"\"\"Main file to run the crash_analysis.py script\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimary_person_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdamages_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharges_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrash_analysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmales_killed_greater_than_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwo_wheeler_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_5_car_crash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mvalid_driver_license_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_with_highest_accidents_no_females\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvehicle_models_with_most_injurie\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Github/case_study_bcg/src/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BCG_Crash_Analysis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mprimary_person_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary_person_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0munits_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/Units_use.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mdamages_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/Damages_use.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/Users/muriel/Documents/Github/case_study_bcg/data/data/Primary_Person_use.csv."
     ]
    }
   ],
   "source": [
    "\"\"\"Script for crash analysis.\"\"\"\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "\n",
    "def males_killed_greater_than_2(primary_person_df):\n",
    "    \"\"\"\n",
    "    A function to return the number of males killed in an accident.\n",
    "    :param primary_person_df: References the Primary_person_use.csv dataset\n",
    "    :return: Returns the row count of the dataframe.\n",
    "    \"\"\"\n",
    "    crash_df = primary_person_df.filter((col('PRSN_GNDR_ID') == 'MALE') & (col('DEATH_CNT') > 2))\n",
    "    return crash_df.count()\n",
    "\n",
    "\n",
    "def two_wheeler_count(units_df):\n",
    "    \"\"\"\n",
    "    A function to return the number of two-wheelers involved in\\\n",
    "     the accident matching motorcycles or police motocycles.\n",
    "    :param units_df: References the Units_use.csv dataset\n",
    "    :return: Returns the count of two-wheelers.\n",
    "    \"\"\"\n",
    "    two_wheeler_df = units_df.filter(\n",
    "        (col('VEH_BODY_STYL_ID') == 'MOTORCYCLE') | (col('VEH_BODY_STYL_ID') == 'POLICE MOTORCYCLE'))\n",
    "    return two_wheeler_df.count()\n",
    "\n",
    "\n",
    "def top_5_car_crash(person_df, units_df):\n",
    "    \"\"\"\n",
    "    A function to return the top 5 car models reporting deaths and air-bags not deployed.\n",
    "    :param units_df: References the Units_use.csv dataset\n",
    "    :param person_df: References the Primary_person_use.csv dataset\n",
    "    :return: Returns the names of car models.\n",
    "    \"\"\"\n",
    "    vehicle_df = (person_df.join(units_df, on='CRASH_ID', how='inner')\n",
    "                  .filter((col('PRSN_AIRBAG_ID') == 'NOT DEPLOYED')\n",
    "                          & (col('PRSN_INJRY_SEV_ID') == 'KILLED')\n",
    "                          & (col('PRSN_TYPE_ID') == 'DRIVER')\n",
    "                          & (col('VEH_MAKE_ID') != 'NA'))\n",
    "                  .groupby('VEH_MAKE_ID').count()\n",
    "                  .orderBy('count', ascending=False)\n",
    "                  .limit(5))\n",
    "\n",
    "    return vehicle_df.select(col('VEH_MAKE_ID'))\n",
    "\n",
    "\n",
    "def valid_driver_license_count(primary_person_df, units_df):\n",
    "    \"\"\"\n",
    "    A function to get the count of drivers with a valid driver's license involved in a hit-and-run.\n",
    "    :param primary_person_df: References the Primary_person_use.csv dataset\n",
    "    :param units_df: References the Units_use.csv dataset\n",
    "    :return: Returns the count of hit-and-run drivers.\n",
    "    \"\"\"\n",
    "    valid_license_df = (primary_person_df.join(units_df, on='CRASH_ID', how='inner')\n",
    "                        .filter(((col('DRVR_LIC_TYPE_ID') == 'DRIVER LICENSE') |\n",
    "                                 (col('DRVR_LIC_TYPE_ID') == 'COMMERCIAL DRIVER LIC.')) &\n",
    "                                (col('VEH_HNR_FL') == 'Y'))).count()\n",
    "    return valid_license_df\n",
    "\n",
    "\n",
    "def state_with_highest_accidents_no_females(primary_person_df):\n",
    "    \"\"\"\n",
    "    A function to return highest number of accidents in which females are not involved.\n",
    "    :param primary_person_df: References the Primary_person_use.csv dataset\n",
    "    :return: Returns a series for State with the highest count of accidents.\n",
    "    \"\"\"\n",
    "    non_females_df = (primary_person_df.filter(col('PRSN_GNDR_ID') != 'FEMALE')\n",
    "                      .groupby('DRVR_LIC_STATE_ID')\n",
    "                      .count().orderBy('count', ascending=False).limit(1))\n",
    "    return non_females_df.select(col('DRVR_LIC_STATE_ID'))\n",
    "\n",
    "\n",
    "def vehicle_models_with_most_injurie(primary_person_df, units_df):\n",
    "    \"\"\"\n",
    "    A function to report the top 3rd to 5th vehicle models that contribute to\n",
    "     the largest number of injuries including death.\n",
    "    :param primary_person_df: References the Primary_person_use.csv dataset\n",
    "    :param units_df: References the Units_use.csv dataset\n",
    "    :return: Returns a tuple of the 3rd to 5th vehicle model names contributing to injuries and death.\n",
    "    \"\"\"\n",
    "    valid_license_df = (primary_person_df.join(units_df, on='CRASH_ID', how='inner')\n",
    "                        .filter(((col('PRSN_INJRY_SEV_ID') != 'NOT INJURED') |\n",
    "                                 (col('PRSN_INJRY_SEV_ID') != 'UNKNOWN')) &\n",
    "                                (col('VEH_MAKE_ID') != 'NA'))\n",
    "                        .groupby('VEH_MAKE_ID').count().orderBy('count', ascending=False)).collect()\n",
    "\n",
    "    return valid_license_df[2].asDict()['VEH_MAKE_ID'], valid_license_df[4].asDict()['VEH_MAKE_ID']\n",
    "\n",
    "\n",
    "def top_ethnic_group_body_style(primary_person_df, units_df):\n",
    "    \"\"\"\n",
    "    A function to return the top ethnic user group of each unique body style.\n",
    "    :param primary_person_df: References the Primary_person_use.csv dataset\n",
    "    :param units_df: References the Units_use.csv dataset\n",
    "    :return: Returns a dataframe for the top ethnic group.\n",
    "    \"\"\"\n",
    "    primary_person_df = primary_person_df.filter(~primary_person_df.PRSN_ETHNICITY_ID.isin(['UNKNOWN', 'OTHER']))\n",
    "    units_df = units_df.filter(\n",
    "        (~units_df.VEH_BODY_STYL_ID.isin(['NA', 'UNKNOWN', 'OTHER  (EXPLAIN IN NARRATIVE)', 'NOT REPORTED'])))\n",
    "\n",
    "    ethnic_group_df = primary_person_df.join(units_df, on='CRASH_ID', how='inner')\n",
    "    ethnic_group_df = (\n",
    "        ethnic_group_df.groupby('PRSN_ETHNICITY_ID', 'VEH_BODY_STYL_ID').count().orderBy('count', asceding=False)\n",
    "        .withColumn('rn',\n",
    "                    row_number().over(Window.partitionBy('VEH_BODY_STYL_ID').orderBy(col('count').desc()))).filter(\n",
    "            'rn == 1'))\n",
    "    return ethnic_group_df.select(col('PRSN_ETHNICITY_ID'), col('VEH_BODY_STYL_ID'))\n",
    "\n",
    "\n",
    "def crash_due_to_alcohol_by_zip_code(primary_person_df):\n",
    "    \"\"\"\n",
    "    A function to return top 5 Zip Codes with the highest number crashes with alcohols.\n",
    "    :param primary_person_df:  References the Primary_person_use.csv dataset.\n",
    "    :return: Returns a dataframe with zip codes.\n",
    "    \"\"\"\n",
    "    crash_df = primary_person_df.select(col('PRSN_ALC_RSLT_ID'), col('DRVR_ZIP')).filter(col('DRVR_ZIP') != 'NULL')\n",
    "    crash_df = crash_df.filter(col('PRSN_ALC_RSLT_ID') == 'Positive').groupby('DRVR_ZIP').count().orderBy('count',\n",
    "                                                                                                          ascending=False).limit(\n",
    "        5)\n",
    "    return crash_df.select(col('DRVR_ZIP'))\n",
    "\n",
    "\n",
    "def crash_id_with_no_property_damage(damages_df, units_df):\n",
    "    \"\"\"\n",
    "    A function to return Distinct Crash IDs where No Damaged Property\n",
    "    was observed and Damage Level (VEH_DMAG_SCL~) is above 4 and\n",
    "    car avails Insurance.\n",
    "    :param damages_df: References the Damages_use.csv dataset\n",
    "    :param units_df: References the Units_use.csv dataset\n",
    "    :return: Returns a dataframe listing crash IDs.\n",
    "    \"\"\"\n",
    "    property_damage_df = (damages_df.join(units_df, on='CRASH_ID', how='inner')\n",
    "                          .filter(col('FIN_RESP_TYPE_ID')\n",
    "                                  .isin(['PROOF OF LIABILITY INSURANCE', 'LIABILITY INSURANCE POLICY']))\n",
    "                          .filter(~col('VEH_DMAG_SCL_1_ID').isin(['NA', 'NO DAMAGE', 'DAMAGED 1 MINIMUM', 'DAMAGED 2',\n",
    "                                                                  'DAMAGED 3', 'DAMAGED 4', 'INVALID VALUE'])\n",
    "                                  | (~col('VEH_DMAG_SCL_2_ID').isin(\n",
    "        ['NA', 'NO DAMAGE', 'DAMAGED 1 MINIMUM', 'DAMAGED 2',\n",
    "         'DAMAGED 3', 'DAMAGED 4', 'INVALID VALUE'])))\n",
    "                          .filter(col('DAMAGED_PROPERTY') == 'NONE')\n",
    "                          .select(col('CRASH_ID')).distinct()\n",
    "                          )\n",
    "    return property_damage_df.select(col('CRASH_ID')).count()\n",
    "\n",
    "\n",
    "def speeding_offence(primary_person_df, units_df, charges_df):\n",
    "    \"\"\"\n",
    "    A function to return Top 5 Vehicle Makes, charged with speeding related offences,\n",
    "     has licensed Drivers, used top 10 used vehicle colours\n",
    "     and has car licensed with the Top 25 states.\n",
    "    :param charges_df: References the Charges_use.csv dataset\n",
    "    :param primary_person_df: References the Primary_person_use.csv dataset\n",
    "    :param units_df: References the Units_use.csv dataset\n",
    "    :return: Returns a dataframe with top 5 vehicle models.\n",
    "    \"\"\"\n",
    "    speeding_df = charges_df.filter(col('CHARGE').contains('SPEED')).select('CRASH_ID')\n",
    "    color_df = (units_df.groupby('VEH_COLOR_ID').count().orderBy('count', ascending=False)\n",
    "                .filter(col('VEH_COLOR_ID') != 'NA').limit(10).select('VEH_COLOR_ID'))\n",
    "\n",
    "    vehicles_top_colors_df = units_df.join(color_df, 'VEH_COLOR_ID')\n",
    "\n",
    "    licensed_df = primary_person_df.filter(col('DRVR_LIC_TYPE_ID').isin(['DRIVER LICENSE', 'COMMERCIAL DRIVER LIC.']))\n",
    "\n",
    "    state_offenses_df = primary_person_df.groupBy('DRVR_LIC_STATE_ID').count().orderBy('count', ascending=False).limit(25)\n",
    "    top_25_states_list = state_offenses_df.select('DRVR_LIC_STATE_ID').rdd.flatMap(lambda x: x).collect()\n",
    "    top_25_states = (primary_person_df.filter(col('DRVR_LIC_STATE_ID').isin(top_25_states_list))\n",
    "                     .groupby('DRVR_LIC_STATE_ID').count()\n",
    "                     .orderBy('count', ascending=False).limit(25))\n",
    "\n",
    "    drivers_in_top_states_df = primary_person_df.join(top_25_states, 'DRVR_LIC_STATE_ID')\n",
    "\n",
    "    joined_df = (speeding_df.join(licensed_df, 'CRASH_ID')\n",
    "                 .join(vehicles_top_colors_df, 'CRASH_ID')\n",
    "                 .join(drivers_in_top_states_df, 'CRASH_ID'))\n",
    "\n",
    "    top_5_vehicle_models = (joined_df.groupby('VEH_MAKE_ID').count()\n",
    "                           .orderBy('count', ascending=False).limit(5))\n",
    "\n",
    "    return top_5_vehicle_models.select(col('VEH_MAKE_ID'))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Main file to run the crash_analysis.py script\"\"\"\n",
    "from src import spark, primary_person_path, units_path, damages_path, charges_path\n",
    "from src.crash_analysis import males_killed_greater_than_2, two_wheeler_count, top_5_car_crash, \\\n",
    "    valid_driver_license_count, state_with_highest_accidents_no_females, vehicle_models_with_most_injurie, \\\n",
    "    top_ethnic_group_body_style, crash_due_to_alcohol_by_zip_code, crash_id_with_no_property_damage, speeding_offence\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Q1:\n",
    "    cnt_death = males_killed_greater_than_2(primary_person_path)\n",
    "    print(f'Number of males death greater than 2: {cnt_death}')\n",
    "\n",
    "    # Q2:\n",
    "    two_wheeler_crash = two_wheeler_count(units_path)\n",
    "    print(f'Count of two wheelers booked for crashes: {two_wheeler_crash}')\n",
    "\n",
    "\n",
    "    # Q3:\n",
    "    print(f'Top 5 Vehicle Makes of the cars present in the crashes in which driver died and Airbags did not deploy:')\n",
    "    top_5_car_crash(primary_person_path, units_path).show()\n",
    "\n",
    "    # Q4\n",
    "    licence_cnt = valid_driver_license_count(primary_person_path, units_path)\n",
    "    print(f'Number of Vehicles with driver having valid licences involved in hit and run: {licence_cnt}')\n",
    "\n",
    "    # Q5\n",
    "    print(f'State with the highest number of accidents in which females are not involved: ')\n",
    "    state_with_highest_accidents_no_females(primary_person_path).show()\n",
    "\n",
    "    # Q6\n",
    "    most_injuries = vehicle_models_with_most_injurie(primary_person_path, units_path)\n",
    "    print(f'Top 3rd to 5th VEH_MAKE_IDs that contribute to a '\n",
    "          f'largest number of injuries including death: {most_injuries}')\n",
    "\n",
    "    # Q7\n",
    "    print('Top ethnic user group of each unique body style:')\n",
    "    top_ethnic_group_body_style(primary_person_path, units_path).show()\n",
    "\n",
    "    # Q8:\n",
    "    print('Top 5 Zip Codes with highest number crashes with '\n",
    "          'alcohols as the contributing factor to a crash:')\n",
    "    crash_due_to_alcohol_by_zip_code(primary_person_path).show()\n",
    "\n",
    "    # Q9:\n",
    "    print(f'Count of Distinct Crash IDs where No Damaged Property was observed '\n",
    "          f'and Damage Level (VEH_DMAG_SCL~) is above 4 and car avails Insurance:'\n",
    "          f' {crash_id_with_no_property_damage(damages_path, units_path)}')\n",
    "\n",
    "    # Q10:\n",
    "    print(f'Top 5 Vehicle Makes where drivers are charged with speeding '\n",
    "          f'related offences, has licensed Drivers, used top 10 used vehicle '\n",
    "          f'colours and has car licensed with the Top 25 states with highest number of offences: ')\n",
    "    speeding_offence(primary_person_path, units_path, charges_path).show()\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
